{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step is loading our pickel files for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "infile = open(\"BSDS_15.pkl\",'rb')\n",
    "dataset = pickle.load(infile) #x,y or clean, noisety\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[0]=first x y pair\n",
    "#dataset[1]=second x y pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_loader=torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the dataset is prepared and shuffled, we can call the model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSDS300      CA6-TEMPLATE.ipynb  dncnn15_0point000001.pt\r\n",
      "BSDS_15.pkl  Dataset.ipynb\t my_env\r\n",
      "BSDS_25.pkl  Dataset_IVP.ipynb\t ondemand\r\n",
      "BSDS_50.pkl  DnCNN_IVP.ipynb\t project1_model.pt\r\n",
      "BSDS_OG.pkl  data\t\t training_dnCNN_IVP.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install import-ipynb\n",
    "#https://stackoverflow.com/questions/20186344/importing-an-ipynb-file-from-another-ipynb-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from DnCNN_IVP.ipynb\n"
     ]
    }
   ],
   "source": [
    "import DnCNN_IVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DnCNN(\n",
       "  (c1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (c2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (b2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now that it is imported, let us create an instance\n",
    "model=DnCNN_IVP.DnCNN(1,1,20)\n",
    "device=torch.device('cuda:0')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 180, 180]             576\n",
      "            Conv2d-2         [-1, 64, 180, 180]          36,928\n",
      "       BatchNorm2d-3         [-1, 64, 180, 180]             128\n",
      "            Conv2d-4         [-1, 64, 180, 180]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 180, 180]             128\n",
      "            Conv2d-6         [-1, 64, 180, 180]          36,928\n",
      "       BatchNorm2d-7         [-1, 64, 180, 180]             128\n",
      "            Conv2d-8         [-1, 64, 180, 180]          36,928\n",
      "       BatchNorm2d-9         [-1, 64, 180, 180]             128\n",
      "           Conv2d-10         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-11         [-1, 64, 180, 180]             128\n",
      "           Conv2d-12         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-13         [-1, 64, 180, 180]             128\n",
      "           Conv2d-14         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-15         [-1, 64, 180, 180]             128\n",
      "           Conv2d-16         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-17         [-1, 64, 180, 180]             128\n",
      "           Conv2d-18         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-19         [-1, 64, 180, 180]             128\n",
      "           Conv2d-20         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-21         [-1, 64, 180, 180]             128\n",
      "           Conv2d-22         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-23         [-1, 64, 180, 180]             128\n",
      "           Conv2d-24         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-25         [-1, 64, 180, 180]             128\n",
      "           Conv2d-26         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-27         [-1, 64, 180, 180]             128\n",
      "           Conv2d-28         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-29         [-1, 64, 180, 180]             128\n",
      "           Conv2d-30         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-31         [-1, 64, 180, 180]             128\n",
      "           Conv2d-32         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-33         [-1, 64, 180, 180]             128\n",
      "           Conv2d-34         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-35         [-1, 64, 180, 180]             128\n",
      "           Conv2d-36         [-1, 64, 180, 180]          36,928\n",
      "      BatchNorm2d-37         [-1, 64, 180, 180]             128\n",
      "           Conv2d-38          [-1, 1, 180, 180]             577\n",
      "================================================================\n",
      "Total params: 668,161\n",
      "Trainable params: 668,161\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 585.60\n",
      "Params size (MB): 2.55\n",
      "Estimated Total Size (MB): 588.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,input_size=(1,180,180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=200\n",
    "from torch import nn\n",
    "class loss_new(nn.Module): #N is dataset size\n",
    "  def __init__(self):\n",
    "    super(loss_new,self).__init__()\n",
    "  def forward(self,out,y,x):\n",
    "    return torch.norm(out-(y-x))/(2*N) #Where does N get defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=loss_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to set a batch size of 128\n",
    "#Tensor to data loader\n",
    "def training_function(net, loader, optim, epochs=50):\n",
    "  loss_list=[]\n",
    "  #net.train()\n",
    "  \"\"\"\n",
    "    y is the noisy image and it is used as input\n",
    "\n",
    "    x is the noiseless image and it is used as a target label\n",
    "  \"\"\"\n",
    "  for e in range(epochs): #For each epoch\n",
    "    train_loss=0\n",
    "    for clean,noisy in loader:\n",
    "      noisy = noisy.to(device)\n",
    "      clean = noisy.to(device)\n",
    "      prediction = net(noisy)\n",
    "      batch_loss = criterion(prediction, noisy, clean)\n",
    "      optim.zero_grad()#zero grad so they don't stack\n",
    "      batch_loss.backward()\n",
    "      optim.step()\n",
    "      train_loss += batch_loss.item()\n",
    "    train_loss = train_loss/len(loader)\n",
    "    loss_list.append(train_loss)\n",
    "\n",
    "    print (\"Epoch {}: Has a loss of Loss: {:.3f}\".format(e+1 ,train_loss))\n",
    "  return loss_list\n",
    "  #return np.mean(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(1.2260, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(1.1785, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(1.1192, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(1.0260, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.9500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.8589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.4148, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 1: Has a loss of Loss: 0.968\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.7960, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.8033, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.8218, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.8493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.8760, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.8699, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.4342, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 2: Has a loss of Loss: 0.779\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.8366, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.8115, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.7741, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.7371, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.7129, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.7052, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.3459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 3: Has a loss of Loss: 0.703\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.6957, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.7034, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.6966, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.6735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.6588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.6430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.3077, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 4: Has a loss of Loss: 0.626\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.5965, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.5788, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.5756, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.5780, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.5608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.5464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 5: Has a loss of Loss: 0.529\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.5174, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.5006, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.4798, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.4758, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.4624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.4528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 6: Has a loss of Loss: 0.442\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.4095, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.4109, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.3805, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.3705, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.3548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.3400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1689, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 7: Has a loss of Loss: 0.348\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.3063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2930, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2752, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2723, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1329, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 8: Has a loss of Loss: 0.254\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.2234, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2145, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1924, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.2008, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1923, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0981, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 9: Has a loss of Loss: 0.190\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.1787, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1743, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1693, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0780, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 10: Has a loss of Loss: 0.153\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.1475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1328, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 11: Has a loss of Loss: 0.130\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.1255, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1246, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1207, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1279, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1217, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 12: Has a loss of Loss: 0.114\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.1149, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1077, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1140, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1049, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1067, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1071, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 13: Has a loss of Loss: 0.103\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.1029, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1008, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.1007, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0913, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0936, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 14: Has a loss of Loss: 0.092\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0924, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0928, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0877, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0847, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0918, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0899, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 15: Has a loss of Loss: 0.084\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0854, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0858, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0850, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0875, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0861, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0808, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 16: Has a loss of Loss: 0.079\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0820, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0855, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0789, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0796, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0805, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0810, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 17: Has a loss of Loss: 0.076\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0708, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0764, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0755, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0765, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 18: Has a loss of Loss: 0.070\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0752, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0728, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0736, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0717, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0719, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0346, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 19: Has a loss of Loss: 0.067\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0680, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0689, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0687, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0322, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 20: Has a loss of Loss: 0.062\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0667, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0309, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 21: Has a loss of Loss: 0.061\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0354, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 22: Has a loss of Loss: 0.059\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0291, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 23: Has a loss of Loss: 0.055\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0265, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 24: Has a loss of Loss: 0.055\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0308, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 25: Has a loss of Loss: 0.051\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0296, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 26: Has a loss of Loss: 0.054\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0311, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 27: Has a loss of Loss: 0.054\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0747, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0771, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0269, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 28: Has a loss of Loss: 0.060\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0699, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0678, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0244, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 29: Has a loss of Loss: 0.055\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0271, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 30: Has a loss of Loss: 0.051\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0705, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0284, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 31: Has a loss of Loss: 0.055\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0753, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0728, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0723, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0256, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 32: Has a loss of Loss: 0.059\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0267, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 33: Has a loss of Loss: 0.053\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0271, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 34: Has a loss of Loss: 0.052\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0786, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0679, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0239, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 35: Has a loss of Loss: 0.057\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0801, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0693, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0694, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0206, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 36: Has a loss of Loss: 0.058\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0246, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 37: Has a loss of Loss: 0.047\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0736, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0699, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0214, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 38: Has a loss of Loss: 0.055\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0702, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0217, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 39: Has a loss of Loss: 0.054\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0717, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0695, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0219, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 40: Has a loss of Loss: 0.054\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0706, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0184, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 41: Has a loss of Loss: 0.055\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0667, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0223, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 42: Has a loss of Loss: 0.051\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0687, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0238, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 43: Has a loss of Loss: 0.053\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0710, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0219, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 44: Has a loss of Loss: 0.055\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0256, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 45: Has a loss of Loss: 0.051\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0811, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0704, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0182, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 46: Has a loss of Loss: 0.056\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0671, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0318, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 47: Has a loss of Loss: 0.051\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0842, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0839, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0935, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0933, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 48: Has a loss of Loss: 0.069\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0726, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0901, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0879, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0934, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0204, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 49: Has a loss of Loss: 0.067\n",
      "epoch change, back to 0?  0\n",
      "Predicted loss for the batch:  tensor(0.0866, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0799, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Predicted loss for the batch:  tensor(0.0320, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 50: Has a loss of Loss: 0.059\n"
     ]
    }
   ],
   "source": [
    "plot_me=training_function(model,train_loader,optimizer,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'learning rate of 0.0001')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(1,51)], plot_me)\n",
    "plt.title(\"learning rate of 0.0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't use a learning rate as high as theirs because we didn't take patches so we have less data and if we just learn super fast, model will explode and oscilate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"dncnn15_0point0001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded=torch.load(\"dncnn15_0point0001.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 zeros slow\n",
    "#3 zeros great\n",
    "#1 zero explodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DnCNN(\n",
       "  (c1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (c2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (b2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=DnCNN_IVP.DnCNN(1,1,20)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=loss_new()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"BSDS_25.pkl\",'rb')\n",
    "dataset = pickle.load(infile) #x,y or clean, noisety\n",
    "infile.close()\n",
    "train_loader=torch.utils.data.DataLoader(dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Has a loss of Loss: 0.810\n",
      "Epoch 2: Has a loss of Loss: 0.732\n",
      "Epoch 3: Has a loss of Loss: 0.642\n",
      "Epoch 4: Has a loss of Loss: 0.566\n",
      "Epoch 5: Has a loss of Loss: 0.487\n",
      "Epoch 6: Has a loss of Loss: 0.406\n",
      "Epoch 7: Has a loss of Loss: 0.287\n",
      "Epoch 8: Has a loss of Loss: 0.156\n",
      "Epoch 9: Has a loss of Loss: 0.092\n",
      "Epoch 10: Has a loss of Loss: 0.069\n",
      "Epoch 11: Has a loss of Loss: 0.063\n",
      "Epoch 12: Has a loss of Loss: 0.064\n",
      "Epoch 13: Has a loss of Loss: 0.052\n",
      "Epoch 14: Has a loss of Loss: 0.059\n",
      "Epoch 15: Has a loss of Loss: 0.048\n",
      "Epoch 16: Has a loss of Loss: 0.086\n",
      "Epoch 17: Has a loss of Loss: 0.086\n",
      "Epoch 18: Has a loss of Loss: 0.085\n",
      "Epoch 19: Has a loss of Loss: 0.043\n",
      "Epoch 20: Has a loss of Loss: 0.059\n",
      "Epoch 21: Has a loss of Loss: 0.064\n",
      "Epoch 22: Has a loss of Loss: 0.088\n",
      "Epoch 23: Has a loss of Loss: 0.090\n",
      "Epoch 24: Has a loss of Loss: 0.085\n",
      "Epoch 25: Has a loss of Loss: 0.154\n",
      "Epoch 26: Has a loss of Loss: 0.088\n",
      "Epoch 27: Has a loss of Loss: 0.084\n",
      "Epoch 28: Has a loss of Loss: 0.078\n",
      "Epoch 29: Has a loss of Loss: 0.048\n",
      "Epoch 30: Has a loss of Loss: 0.084\n",
      "Epoch 31: Has a loss of Loss: 0.041\n",
      "Epoch 32: Has a loss of Loss: 0.044\n",
      "Epoch 33: Has a loss of Loss: 0.081\n",
      "Epoch 34: Has a loss of Loss: 0.106\n",
      "Epoch 35: Has a loss of Loss: 0.155\n",
      "Epoch 36: Has a loss of Loss: 0.217\n",
      "Epoch 37: Has a loss of Loss: 0.388\n",
      "Epoch 38: Has a loss of Loss: 0.221\n",
      "Epoch 39: Has a loss of Loss: 0.214\n",
      "Epoch 40: Has a loss of Loss: 0.085\n",
      "Epoch 41: Has a loss of Loss: 0.067\n",
      "Epoch 42: Has a loss of Loss: 0.050\n",
      "Epoch 43: Has a loss of Loss: 0.054\n",
      "Epoch 44: Has a loss of Loss: 0.041\n",
      "Epoch 45: Has a loss of Loss: 0.058\n",
      "Epoch 46: Has a loss of Loss: 0.040\n",
      "Epoch 47: Has a loss of Loss: 0.043\n",
      "Epoch 48: Has a loss of Loss: 0.066\n",
      "Epoch 49: Has a loss of Loss: 0.070\n",
      "Epoch 50: Has a loss of Loss: 0.066\n"
     ]
    }
   ],
   "source": [
    "plot_me=training_function(model,train_loader,optimizer,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
